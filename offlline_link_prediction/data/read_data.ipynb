{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocesss the csv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3031780, 2)\n",
      "576287 576287 3031780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(507618, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the .pt file NOTE: process train/valid/test each time \n",
    "pt_tensor = torch.load('train.pt')\n",
    "pt_tensor = torch.load('valid.pt')\n",
    "pt_tensor = torch.load('test.pt')\n",
    "\n",
    "# Convert to NumPy array\n",
    "data = np.array(pt_tensor['edge'])\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "np.save(\"collab_train.npy\", data)\n",
    "np.save(\"collab_valid.npy\", data)\n",
    "np.save(\"collab_test.npy\", data)\n",
    "\n",
    "data_array = np.load(\"collab_train.npy\")\n",
    "data_array = np.load(\"collab_valid.npy\")\n",
    "data_array = np.load(\"collab_test.npy\")\n",
    "\n",
    "# find the max user and item idx\n",
    "max_item_idx = np.max(data_array[:, 1])\n",
    "max_user_idx = np.max(data_array[:, 0])\n",
    "print(max_item_idx,max_user_idx, data_array.shape[0])\n",
    "data_array = data_array.astype(int)\n",
    "\n",
    "ones_column = np.ones((data_array.shape[0], 1))\n",
    "new_data_array = np.hstack((data_array, ones_column))\n",
    "\n",
    "condition = (new_data_array[:, 0] < 235868) & (new_data_array[:, 1] < 235868) #& (raw_array[:, 2] == 1)\n",
    "raw_10000 = new_data_array[condition]\n",
    "raw_10000 = np.array(raw_10000)\n",
    "\n",
    "condition = (new_data_array[:, 0] < 235868) & (new_data_array[:, 1] < 235868) & (new_data_array[:, 2] == 1)\n",
    "raw_10000_edge = new_data_array[condition]\n",
    "raw_10000_edge = np.array(raw_10000_edge)\n",
    "raw_10000_edge.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modify the row dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disconnected_edge(total_edge_num, connected_edge):\n",
    "    num_disconnected_edges = total_edge_num - connected_edge.shape[0]\n",
    "    existing_edges_set = set(map(tuple, connected_edge))\n",
    "    disconnected_edges = set()\n",
    "\n",
    "    while len(disconnected_edges) < num_disconnected_edges:\n",
    "        edge = tuple(np.random.randint(0, 235868, size=2, dtype=np.int32))\n",
    "        edge_extend = edge + (1,)\n",
    "        if edge[0] != edge[1] and edge_extend not in existing_edges_set:\n",
    "            disconnected_edges.add(edge)\n",
    "\n",
    "    # Convert disconnected_edges to a numpy array\n",
    "    disconnected_edges = np.array(list(disconnected_edges), dtype=np.int32)\n",
    "\n",
    "    # Create arrays for all original and disconnected edges\n",
    "    disconnected_edges =  np.column_stack((disconnected_edges, -1 * np.ones(num_disconnected_edges, dtype=np.int32)))\n",
    "\n",
    "    return disconnected_edges.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_disconnect_edges = add_disconnected_edge(len(raw_10000_edge) * 2, raw_10000_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE:process train/valid/test seperately for each run \n",
    "total_trained_edge = np.vstack((raw_10000_edge,total_disconnect_edges)).astype(int)\n",
    "total_valid_edge = np.vstack((raw_10000_edge,total_disconnect_edges)).astype(int)\n",
    "total_test_edge =np.vstack((raw_10000_edge,total_disconnect_edges)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the pre processed dataset to npy form \n",
    "np.save(\"./collab_users_entry_train.npy\", total_trained_edge)\n",
    "np.save(\"./collab_users_entry_valid.npy\", total_valid_edge)\n",
    "np.save(\"./collab_users_entry_test.npy\", total_test_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3045708, 3)\n",
      "[[     0 131037     -1]\n",
      " [     0 131037     -1]\n",
      " [     0 131037     -1]\n",
      " [     1  31768     -1]\n",
      " [     1  31768     -1]\n",
      " [     1  31768     -1]\n",
      " [     1  78236     -1]\n",
      " [     1  78236     -1]\n",
      " [     1  78236     -1]\n",
      " [     2   1910     -1]]\n"
     ]
    }
   ],
   "source": [
    "total_edge = np.concatenate((total_trained_edge, total_valid_edge, total_test_edge), axis = 0)\n",
    "sorted_indices = np.lexsort((total_edge[:, 1], total_edge[:, 0]))\n",
    "sorted_edges = total_edge[sorted_indices]\n",
    "print(sorted_edges.shape)\n",
    "print(sorted_edges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graph with no edges\n",
    "import numpy as np\n",
    "G = sorted_edges[:]\n",
    "for i in range(G.shape[0]): G[i,2] = 0\n",
    "np.save(\"./collab_users_noedge.npy\", G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original file starting now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import scipy as sp\n",
    "import scipy.sparse as spp\n",
    "\n",
    "def load_sparse_matrix(filename):\n",
    "    t = 0\n",
    "    f = open(filename, 'r')\n",
    "    f.readline()\n",
    "\n",
    "    user_list = []\n",
    "    item_list = []\n",
    "    data_list = []\n",
    "    count  = 0\n",
    "    for line in f:\n",
    "        word = line.split(',')\n",
    "       \n",
    "        rating = float(word[2])\n",
    "\n",
    "        t += 1\n",
    "        if t % 50000 == 0:\n",
    "            print('.', end = '')\n",
    "            if t % 2000000 == 0:\n",
    "                print('%d m lines' %(t // 1000000))\n",
    "            #data_list.append(float(rating))\n",
    "        if rating > 4.0:\n",
    "            data_list.append(1)\n",
    "        elif rating < 2.0:\n",
    "            data_list.append(-1)\n",
    "            count +=1\n",
    "        else: \n",
    "            continue    \n",
    "        \n",
    "        user_list.append(int(word[0]))\n",
    "        item_list.append(int(word[1]))\n",
    "        \n",
    "    print(count)\n",
    "    users = list(set(user_list))\n",
    "    items = list(set(item_list))\n",
    "    user2id = dict(zip(users, range(len(users))))\n",
    "    item2id = dict(zip(items, range(len(items))))\n",
    "\n",
    "    user_id_list = [user2id[u] for u in user_list]\n",
    "    item_id_list = [item2id[i] for i in item_list]\n",
    "\n",
    "    result = spp.csr_matrix((data_list, (user_id_list, item_id_list)))\n",
    "    return result\n",
    "\n",
    "def extract_users(num_users, sparse_matrix):\n",
    "\titem_count = sparse_matrix.getnnz(axis = 1)\n",
    "\tuser_sort = sorted(enumerate(item_count), key = lambda x: x[1], reverse = True)[:num_users]\n",
    "\tuser_indices = [x[0] for x in user_sort]\n",
    "\n",
    "\treturn spp.vstack([sparse_matrix.getrow(i) for i in user_indices])\n",
    "\n",
    "def extract_items(num_items, sparse_matrix):\n",
    "\tuser_count = sparse_matrix.getnnz(axis = 0)\n",
    "\titem_sort = sorted(enumerate(user_count), key = lambda x: x[1], reverse = True)[:num_items]\n",
    "\titem_indics = [x[0] for x in item_sort]\n",
    "\n",
    "\treturn spp.hstack([sparse_matrix.getcol(j) for j in item_indics])\n",
    "\n",
    "def get_reduced_matrix(num_users, num_items, filename):\n",
    "\tdata = load_sparse_matrix(filename)\n",
    "\tdata1 = extract_users(3 * num_users, data)\n",
    "\tdata2 = extract_items(num_items, data1)\n",
    "\tdata3 = extract_users(num_users, data2)\n",
    "\treturn data3.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015236, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edges_ = np.load(\"./collab_users_entry_train.npy\")\n",
    "train_edges_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_adjacency_matrix_from_G(G, num_users,num_items):\n",
    "    num_nodes = num_users \n",
    "    A = spp.lil_matrix((num_nodes, num_nodes))\n",
    "    for i in range(len(G)):\n",
    "        user, item, weight = G[i]\n",
    "        if weight == 1:\n",
    "            A[user, item] = 1\n",
    "        else: A[user , item] = -1\n",
    "    return A\n",
    "# G = np.load(\"GrQc_ALLusers_entry.npy\")\n",
    "A = construct_adjacency_matrix_from_G(train_edges_,235868,235868)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235868, 235868)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_ = A.tocsr()\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def ExtractFeatures(num, d, m):\n",
    "    A1 = m[:num, :]  # giữ nguyên m là sparse matrix (CSR format)\n",
    "    u, s, vt = svds(A1, k=d-1)  # sparse SVD\n",
    "    u = normalize(u, axis=1, norm='l2')\n",
    "    U = np.concatenate((u, np.ones((num, 1))), axis=1) / np.sqrt(2)\n",
    "    return U\n",
    "\n",
    "U = ExtractFeatures(235868, d=10, m = A)\n",
    "np.save(\"./collab_users_items_features.npy\", U)\n",
    "I = ExtractFeatures(235868, d=10, m = A.T)\n",
    "np.save(\"./collab_items_users_features.npy\", I)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
